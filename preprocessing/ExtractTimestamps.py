import os
import pandas as pd
import json
from os.path import dirname

from regex import F
from env import LOG_DIRECTORY


# Function to extract the timestamps from the Fault Injection Loop log file
def extract_FIL_data():
    LOG_FILE = f"{LOG_DIRECTORY}/HDD_Fault_injection-loop.log"

    LOG_VALUES = []
    ENTRY = []
    FLAG = False

    with open(f"{LOG_FILE}", "r") as Log:
        lines = Log.readlines()

        for line in lines:
            if "Starting experiment" in line:
                FLAG = True
            if (
                ("Starting HDD Unrecoverable Read Error injection..." in line)
                or ("Start continuous injection" in line)
                and FLAG
            ):
                ENTRY.append(line.split(" ")[0][:-1].replace("_", " ") + ".000000")
            if "End experiment" in line:
                FLAG = False
                LOG_VALUES.append(ENTRY)
                ENTRY = []

    return LOG_VALUES


# Function to extract the exact timestamp and corresponding index and value from the csv file
def exact_timestamp(timestamp, limit):
    # Filter the dataframe and find the records with FIL log timestamp
    DF = DATAFRAME.loc[DATAFRAME["timestamp"].str.contains(timestamp)]

    ts = pd.to_datetime(timestamp)

    # If the FIL log timestamp is not found in the dataframe, then we have to find the nearest timestamp
    # First we try to find a timestamp before the FIL log timestamp, if not found within 2 seconds range then we try to find a timestamp after the FIL log timestamp
    rounds = 1
    flag = False
    while len(DF.index) == 0:
        if rounds > limit:
            ts += pd.Timedelta(seconds=3)
            flag = True
        else:
            ts -= pd.Timedelta(seconds=1)

        DF = DATAFRAME.loc[DATAFRAME["timestamp"].str.contains(str(ts))]
        rounds += 1

        if flag:
            break

    length = len(DF.index)

    # If timestamp after the FIL log timestamp is not found, then we return False
    if length == 0:
        return False

    index = DF.index[0]

    # If multiple timestamps are found, then we take the first timestamp subtract 1 second and add .001 millisecond to make it unique and select it as the exact timestamp
    timestamp = DF.iloc[0]["timestamp"]
    if length > 1:
        timestamp = pd.to_datetime(DF.iloc[0]["timestamp"])
        timestamp = timestamp + pd.Timedelta(seconds=-1, milliseconds=0.001)

    value = DF.iloc[0]["value"]

    return [int(index), str(timestamp), int(value)]


# Main Function
# Output files generated by logPreprocessing
DATA_DIRECTORY = f"{dirname(__file__)}/final-output/hdd"
# Output directory to store the csv files with the exact timestamps
OUTPUT_DIRECTORY = f"{dirname(__file__)}/../data/hdd"

FIL_LOG_VALUES = extract_FIL_data()

COUNT = 0
LABEL_REGION = {}
FAILURE_POINT = {}
DATA = {}

# Iterate through all the csv files in the directory
for FILE in os.scandir(DATA_DIRECTORY):
    # Skip the files that are not csv files
    if not FILE.name.endswith(".csv"):
        continue

    FILE_NAME = FILE.name.split(".")[0]

    # Open the csv file
    with open(FILE.path, "r") as CSV:
        # Read the csv file into a dataframe
        DATAFRAME = pd.read_csv(CSV)

        # Add .000000 milliseconds to the timestamps
        DATAFRAME["timestamp"] = DATAFRAME["timestamp"] + ".000000"

        # Get the [index, timestamp, value] of the label region and failure point
        # exact_timestamp(timestamp, limit) where timestamp is the FIL log timestamp and limit is the number of seconds to be added/subtracted to find the exact timestamp
        # LOCAL REGION
        LR = exact_timestamp(FIL_LOG_VALUES[COUNT][0], 2)
        # FAILURE POINT
        FP = exact_timestamp(FIL_LOG_VALUES[COUNT][1], 2)

        print(FP, LR)

        # If a suitable point is not found, then DATAFRAME is not updated
        if LR and FP:
            DATAFRAME.loc[DATAFRAME.index[LR[0]]] = [LR[1], LR[2]]
            DATAFRAME.loc[DATAFRAME.index[FP[0]]] = [FP[1], FP[2]]
            DATAFRAME.to_csv(f"{OUTPUT_DIRECTORY}/{FILE.name}", index=False)
        if not LR:
            LR = [-1, "Not Found", "Not Found"]
        if not FP:
            FP = [-1, "Not Found", "Not Found"]

        print(FP, LR)

        # Update the metadata
        DATA[FILE_NAME] = {
            "comment": "HDD",
            "label_region": LR[0] + 1,
            "label_region_timestamp": LR[1],
            "failure_point": FP[0] + 1,
            "failure_point_timestamp": FP[1],
        }

    COUNT += 1

# Write the metadata to a json file
with open(f"{OUTPUT_DIRECTORY}/dataset_metadata.json", "w") as JSON:
    json.dump(DATA, JSON)
