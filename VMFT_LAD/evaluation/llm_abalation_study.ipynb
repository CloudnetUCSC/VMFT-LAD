{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from drain3.file_persistence import FilePersistence\n",
    "\n",
    "from log_inference_providers.BartLarge.FastBartLargeZeroShotLogInference import (\n",
    "    FastBartLargeZeroShotLogInference,\n",
    ")\n",
    "from log_inference_providers.Falcon7B.Falcon7BFewShotLogInference import (\n",
    "    FastFalcon7BFewShotLogInference,\n",
    ")\n",
    "from log_inference_providers.FakeLogInferenceProvider import FakeLogInferenceProvider\n",
    "from log_inference_providers.GPT3.FastGPT3FewShotLogInference import (\n",
    "    FastGPT3FewShotLogInference,\n",
    ")\n",
    "from log_inference_providers.EmertonMonarch7B.FastEmertonMonarch7BFewShotLogInference import (\n",
    "    FastEmertonMonarch7BFewShotLogInference,\n",
    ")\n",
    "from log_inference_providers.Cyrax7B.FastCyrax7BFewShotLogInference import (\n",
    "    FastCyrax7BFewShotLogInference,\n",
    ")\n",
    "from log_inference_providers.NullLogInferenceProvider import NullLogInferenceProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drain_config_base_path = f\"../../../preprocessing\"\n",
    "persistence = FilePersistence(f\"{drain_config_base_path}/drain3_state.bin\")\n",
    "config = TemplateMinerConfig()\n",
    "config.load(f\"{drain_config_base_path}/drain3.ini\")\n",
    "config.profiling_enabled = False\n",
    "template_miner = TemplateMiner(persistence, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_providers = {\n",
    "    \"Fake\": FakeLogInferenceProvider(),\n",
    "    \"Null\": NullLogInferenceProvider(),\n",
    "    \"BartLarge\": FastBartLargeZeroShotLogInference(),\n",
    "    \"Falcon7B\": FastFalcon7BFewShotLogInference(),\n",
    "    \"GPT3\": FastGPT3FewShotLogInference(),\n",
    "    \"EmertonMonarch7B\": FastEmertonMonarch7BFewShotLogInference(),\n",
    "    \"Cyrax7B\": FastCyrax7BFewShotLogInference(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Starting inference provider: Falcon7B\n",
      "## Starting dataset: cpu\n",
      "## Starting dataset: buffer-io\n",
      "## Starting dataset: hdd\n",
      "## Starting dataset: oom\n",
      "## Starting dataset: benign\n"
     ]
    }
   ],
   "source": [
    "for inference_provider in inference_providers.keys():\n",
    "    print(\"## Starting inference provider:\", inference_provider)\n",
    "    for dataset_name in [\"cpu\", \"buffer-io\", \"hdd\", \"oom\", \"benign\"]:\n",
    "        dataset_dir = f\"../../../data/{dataset_name}\"\n",
    "        dataset_metadata = json.load(open(f\"{dataset_dir}/dataset_metadata.json\"))\n",
    "\n",
    "        output_dir = f\"../../../results/LLM_only/LLM-{inference_provider}/{dataset_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        print(\"## Starting dataset:\", dataset_name)\n",
    "\n",
    "        for datasetFile in os.scandir(dataset_dir):\n",
    "            if not datasetFile.name.endswith(\".csv\"):\n",
    "                continue\n",
    "            \n",
    "            dataset_df = pd.read_csv(datasetFile.path, header=0)\n",
    "            data = dataset_df[\"value\"].tolist()\n",
    "\n",
    "            dataset_file_name = datasetFile.name.split(\".\")[0]\n",
    "\n",
    "            label_index = dataset_metadata[dataset_file_name][\"label_region\"]\n",
    "            \n",
    "            out_df = dataset_df.copy()\n",
    "\n",
    "            for i in range(len(data)):\n",
    "                log_template = template_miner.drain.id_to_cluster[data[i]].get_template()\n",
    "                anomaly_score = int(inference_providers[inference_provider].infer(log_template))\n",
    "\n",
    "                out_df.at[i, 'anomaly_score'] = anomaly_score\n",
    "                if label_index == 0:\n",
    "                    out_df.at[i, 'label'] = 0\n",
    "                else:\n",
    "                    out_df.at[i, 'label'] = 1 if i >= label_index else 0\n",
    "                        \n",
    "            if dataset_name != \"benign\":\n",
    "                out_df.loc[len(out_df)] = {\"timestamp\": \"2025-01-01 12:12:12.000000\",\n",
    "                                        \"value\": 0,\n",
    "                                        \"anomaly_score\": 1.0,\n",
    "                                        \"label\": 1.0,}\n",
    "            \n",
    "            out_df.to_csv(f\"{output_dir}/LLM-{inference_provider}_{dataset_file_name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
